## HindiGPT Tokenizer (BPE) & Data Preparation Pipeline

A complete from-scratch Hindi tokenizer and large-scale data preparation pipeline built for training HindiGPT, a custom decoder-only Transformer language model.

This repository covers the entire journey from raw noisy Hindi corpora to LLM-ready binary token streams, optimized for efficient large-scale training.

âœ¨ Key Highlights

- ðŸ”¤ Pure Hindi SentencePiece BPE Tokenizer (32K vocab)
- ðŸ§¹ Multi-stage data cleaning & normalization pipeline
- ðŸš« NSFW & noise filtering
- ðŸ“¦ Efficient binary token storage (.bin)
- âš¡ Designed for billion-token scale training
- ðŸ§  Used directly in HindiGPT pretraining

ðŸ”¤ Tokenizer Details
Feature	Value
Tokenizer Type	SentencePiece
Model	BPE (Byte Pair Encoding)
Vocabulary Size	32,768
Language	Hindi (Devanagari only)
Unicode Normalization	NFKC
BOS / EOS / PAD / UNK	Enabled
Byte Fallback	âŒ Disabled

### Tokenizer Training Code
```
spm.SentencePieceTrainer.train(
    input="data/hindi_no_exact_dup.txt",
    model_prefix="hindi_tokenizer_new",
    vocab_size=32768,
    model_type="bpe",
    character_coverage=1.0,
    normalization_rule_name="nfkc",
    input_sentence_size=12_000_000,
    shuffle_input_sentence=True,
    pad_id=0, unk_id=1, bos_id=2, eos_id=3
)
```
âœ” Trained on ~1.7 crore Hindi sentences
âœ” Random sampling ensures full corpus coverage

### Tokenizer Validation
Binary tokens are decoded back to text to ensure correctness:
```
arr = np.fromfile("train_3.bin", dtype=np.uint16)
text = sp.decode(arr[:200].tolist())
```
```
Decoded sample: à¤…à¤¦ ipu à¤à¤• à¤§à¤° à¤†à¤¸ à¤¯à¤¤ à¤ªà¤° à¤•à¤°à¤¤ à¤à¤• à¤…à¤¦ à¤”à¤° à¤µà¤¹ à¤à¤• à¤°à¤£ à¤¸à¤• à¤‰à¤ªà¤¯ à¤”à¤° à¤…à¤¨ à¤¶à¤¯à¤µ à¤°à¤¸ à¤¯à¤¦ à¤¸à¤®à¤• à¤•à¤°à¤£ à¤¸à¤• à¤‰à¤² à¤•à¤­ à¤•à¤­ à¤Ÿà¤° à¤à¤• à¤…à¤¦ à¤ªà¤¶ à¤µà¤¸ à¤…à¤¦ à¤°à¤£
 ipu à¤‰à¤ªà¤¯ à¤¯à¤¹ à¤¤à¤° à¤…à¤² à¤¯à¤¤ à¤®à¤¨à¤® à¤‰à¤¦ à¤¹à¤°à¤£ à¤…à¤¦ à¤ˆà¤¶ à¤µà¤°à¤µ à¤•à¤¥à¤¨ à¤ˆà¤¶ à¤µà¤° à¤¶à¤¬ à¤œà¤—à¤¹ à¤ªà¤¨ à¤”à¤° à¤…à¤¦ à¤¯à¤¤ à¤°à¤¸ à¤ªà¤° à¤…à¤¨à¤¨ à¤†à¤ˆà¤ª à¤…à¤¸ à¤•à¤°à¤¨ à¤…à¤¸à¤®à¤° à¤¥à¤¤ à¤²à¤•à¤° à¤‰à¤¨ à¤ªà¤° à¤•à¤°à¤¤ à¤†à¤¸ à¤à¤• à¤†à¤¸
à¤µà¤¤ à¤·à¤¤ à¤°à¤–à¤¤ à¤…à¤¦ à¤…à¤¬ à¤…à¤¨ à¤¤à¤° à¤¨à¤• à¤‡à¤¥ à¤¶à¤¹à¤° à¤…à¤¦ à¤…à¤¬ à¤…à¤¨ à¤¤à¤° à¤¨à¤• à¤¯à¤¹ à¤¨à¤• à¤…à¤¦ à¤…à¤¬ à¤¶à¤¹à¤° à¤¨à¤—à¤° à¤¦à¤• à¤à¤µ à¤‰à¤¤ à¤¤à¤° à¤‡à¤¸à¤• à¤…à¤¨ à¤¤à¤° à¤¨à¤• à¤”à¤° à¤¯à¤¹ à¤µà¤œà¤µ à¤‡à¤¥ à¤¯à¤¨ à¤à¤¯à¤°à¤² à¤°à¤® à¤¹à¤¬ à¤¯à¤¹ à¤¤à¤µ
à¤¸à¤¹ à¤…à¤« à¤…à¤¨ à¤¶à¤¹à¤° à¤à¤µ à¤à¤¶ à¤à¤µ à¤‰à¤¤ à¤¤à¤° à¤…à¤®à¤° à¤°à¤¦ à¤•à¤°à¤¤ à¤…à¤¨ à¤¤à¤° à¤¨à¤• à¤•à¤ˆ à¤…à¤« à¤°à¤µ à¤¶à¤¦ à¤—à¤¯ à¤…à¤¤ à¤¯à¤¹ à¤à¤¸ à¤®à¤¹ à¤…à¤¨ à¤‡à¤¥ à¤¯à¤¨ à¤à¤¯à¤°à¤² à¤…à¤« à¤Ÿà¤µà¤° à¤®à¤¹à¤¤ à¤µà¤ª à¤¨à¤• à¤¯à¤¹ à¤•à¤ˆ à¤Ÿà¤° à¤‰à¤¡ à¤¹à¤¬ à¤‰à¤¦
à¤¹à¤² à¤•à¤°à¤¤ à¤¯à¤¹ à¤…à¤« à¤®à¤¹ à¤¨à¤š à¤²à¤• à¤°à¤¶ à¤·à¤£ à¤à¤• à¤…à¤¨ à¤¤à¤° à¤¨à¤• à¤…à¤« à¤¯à¤¸ à¤¤à¤¤à¤® à¤¨à¤• à¤à¤• à¤œà¤¹ à¤‡à¤¸ à¤¬à¤¢ à¤¸à¤®à¤¯ à¤¨à¤• à¤°à¤« à¤…à¤¨ à¤…à¤« à¤¬à¤¡ à¤¨à¤• à¤…à¤¨ à¤¤à¤• à¤¯à¤¹ à¤…à¤« à¤¸à¤° à¤¬à¤¡ à¤¨à¤• à¤œà¤¹ à¤Ÿà¤¨
```


### Devanagari-Only Filtering
```
[^\u0900-\u097F\s\.\,\!\?\-]
```
âœ” Removes English, symbols, emojis, scripts
âœ” Keeps pure Hindi


### Wikipedia XML Cleaning
- Removes templates, tags, metadata
- Drops short & boilerplate pages

### Length & Quality Filters
- Minimum length: 40â€“200 chars
- Removes junk, lists, headings

### NSFW / Adult Content Filtering
âœ” Ensures safe & clean pretraining corpus

### Data Shuffling (Linux)

Final corpus is shuffled at OS level for randomness:
```
shuf hindi_final_15GB.txt > hindi_final_15GB_shuffled.txt
```
âœ” Prevents topic clustering
âœ” Improves training stability


### Binary Token Generation

Text â†’ tokens â†’ .bin files for fast training IO
```
token_ids = sp.encode(line, out_type=int)
token_ids.append(EOS)
np.array(tokens, dtype=np.uint16).tofile("train_0.bin")
```
